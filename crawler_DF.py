###############################
# combined crawler
# èª˜åµæ¡¶ + å¤©æ°£è³‡æ–™
###############################

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
import urllib.parse
import requests
import time
import os
import pandas as pd
import numpy as np
from datetime import datetime,timedelta

###############################
# 0. åˆå§‹åŒ– driverï¼ˆå…±ç”¨ï¼‰
###############################
def init_driver(path):
    download_dir = os.path.abspath(path)
    os.makedirs(download_dir, exist_ok=True)
    print("ğŸ“‚ ä¸‹è¼‰è·¯å¾‘ï¼š", download_dir)
    chrome_prefs = {
        "download.default_directory": download_dir,
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
        "safebrowsing.disable_download_protection": True,
    }

    opts = Options()
    opts.add_argument("--headless=new")           # èƒŒæ™¯éœé»˜
    opts.add_argument("--window-size=1920,1080")  # è¦–çª—é–‹å¤§ï¼Œç‰ˆé¢æ¯”è¼ƒæ­£å¸¸
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_experimental_option("prefs", chrome_prefs)
    driver = webdriver.Chrome(options=opts)
    wait = WebDriverWait(driver, 10)
    return driver, wait , download_dir


###########################################
# 1ï¸âƒ£ èª˜åµæ¡¶è³‡æ–™çˆ¬èŸ²
###########################################
def crawl_bucket(driver, wait,download_dir, year_title="114å¹´è‡ºå—å¸‚ç™»é©ç†±èª˜åµæ¡¶ç›£æ¸¬è³‡è¨Š", file_name="bucket_114.csv"):
    print("\n====== ğŸª£ èª˜åµæ¡¶è³‡æ–™çˆ¬å– ======")

    driver.get("https://data.tainan.gov.tw/DataSet/Detail/33a5bbc9-6898-4851-9147-4410f0b2f47e")

    # æ‰¾è©²å¹´åº¦
    link_elem = wait.until(
        EC.presence_of_element_located((
            By.XPATH,
            f'//a[@title="{year_title}"]'
        ))
    )
    link_elem.click()
    print(f"âœ… å·²æˆåŠŸé»é€² {year_title}")

    # æŠ“ CSV
    csv_elem = wait.until(
        EC.presence_of_element_located((
            By.XPATH,
            '//a[contains(text(), "CSV")]'
        ))
    )

    href = csv_elem.get_attribute("href")
    full_url = urllib.parse.urljoin("https://data.tainan.gov.tw", href)

    print("ğŸ”— CSV ä¸‹è¼‰é€£çµï¼š", full_url)

    save_path = os.path.join(download_dir, file_name)

    res = requests.get(full_url, headers={"User-Agent": "Mozilla/5.0"})
    res.raise_for_status()

    with open(save_path, "wb") as f:
        f.write(res.content)

    print("ğŸ“ å·²ä¸‹è¼‰ï¼š", save_path)
    print("ğŸª£ èª˜åµæ¡¶ä¸‹è¼‰å®Œç•¢\n")


###########################################
# 2ï¸âƒ£ å¤©æ°£è³‡æ–™çˆ¬èŸ²
###########################################
def normalize_weather_filename(download_dir, target_filename):
    """
    å°‡è‡ªå‹•åŠ  (1)ã€(2)â€¦ çš„æª”æ¡ˆçµ±ä¸€æ”¹æˆ target_filename
    """
    files = os.listdir(download_dir)

    # æ‰¾å‡ºæ‰€æœ‰åƒ "467410-2025-11" é–‹é ­çš„æª”æ¡ˆ
    candidates = [f for f in files if f.startswith(target_filename.split(".")[0])]

    if not candidates:
        print("âš  æ‰¾ä¸åˆ°ä»»ä½•å¤©æ°£è³‡æ–™æª”æ¡ˆ")
        return None

    # æŒ‰ç…§ä¿®æ”¹æ™‚é–“æ’åºï¼Œæœ€æ–°çš„æ’æœ€å¾Œ
    candidates = sorted(
        candidates,
        key=lambda f: os.path.getmtime(os.path.join(download_dir, f))
    )

    newest_file = candidates[-1]  # æœ€æ–°çš„

    src = os.path.join(download_dir, newest_file)
    dst = os.path.join(download_dir, target_filename)

    # è‹¥ dst å·²å­˜åœ¨ â†’ åˆªæ‰
    if os.path.exists(dst):
        os.remove(dst)

    os.rename(src, dst)
    print(f"ğŸ“ å·²å°‡æœ€æ–°ä¸‹è¼‰æª”æ¡ˆï¼š{newest_file} â†’ {target_filename}")

    return dst

def crawl_weather(driver, wait, download_dir,station="467410", month="2024-10"):



    print("\n====== ğŸŒ¦ï¸ å¤©æ°£è³‡æ–™çˆ¬å– ======")

    driver.get("https://codis.cwa.gov.tw/StationData")

    # åˆ‡åˆ°æ¸¬ç«™æ¸…å–®
    btn_list = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(),"æ¸¬ç«™æ¸…å–®")]')))
    btn_list.click()

    # æ‰¾è¡¨æ ¼
    tbody = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody")))
    scroll_container = tbody

    # æ‰¾ chart icon
    icon = None
    for step in range(40):
        matches = tbody.find_elements(
            By.XPATH,
            (
                f'.//tr[.//td[contains(normalize-space(.), "{station}")]]'
                f'//i[contains(@class,"fa-chart-line")]'
            )
        )
        if matches:
            icon = matches[0]
            driver.execute_script("arguments[0].scrollIntoView(false);", icon)
            driver.execute_script("window.scrollBy(0,200);")
            time.sleep(0.3)
            driver.execute_script("arguments[0].click();", icon)
            print(f"âœ… é»æ“Šæ¸¬ç«™ {station}")
            break

        driver.execute_script(
            "arguments[0].scrollTop = arguments[0].scrollTop + 400;",
            scroll_container
        )
        time.sleep(0.5)

    if icon is None:
        raise RuntimeError("âŒ æ‰¾ä¸åˆ°è©²ç«™ç¢¼ï¼Œå¯èƒ½ä¸å­˜åœ¨")

    # åˆ‡æœˆå ±è¡¨
    month_tab = wait.until(
        EC.element_to_be_clickable((
            By.XPATH,
            '//div[@class="lightbox-tool-menu"]/div[.//div[contains(text(),"æœˆå ±è¡¨(é€æ—¥è³‡æ–™)")]]'
        ))
    )
    driver.execute_script("arguments[0].click();", month_tab)
    print("âœ… å·²åˆ‡åˆ°ã€æœˆå ±è¡¨(é€æ—¥è³‡æ–™)ã€")

    time.sleep(0.5)

    # æ‰¾ lightbox
    lightbox = wait.until(
        EC.presence_of_element_located((
            By.XPATH,
            '//section[contains(@class,"lightbox-tool") and .//label[contains(text(),"æ¸¬ç«™æ™‚åºåœ–å ±è¡¨")]]'
        ))
    )
    print("âœ… æ‰¾åˆ° lightbox")

    # è£œåˆ‡æ¸¬ç«™
    try:
        sel = lightbox.find_element(By.TAG_NAME, "select")
        Select(sel).select_by_value(station)
        print("âœ… æ¸¬ç«™å·²åˆ‡æ›")
        time.sleep(0.4)
    except:
        print("âš  æ¸¬ç«™ä¸‹æ‹‰å¯èƒ½å·²æ­£ç¢º")

    # ä¸‹è¼‰ CSV
    csv_btn = lightbox.find_element(
        By.XPATH,
        './/div[contains(@class,"lightbox-tool-type-ctrl-btn") and contains(text(),"CSV")]'
    )
    driver.execute_script("arguments[0].click();", csv_btn)
    print("ğŸŒ¦ï¸ æ­£åœ¨ä¸‹è¼‰ CSV...")
    now = datetime.now()
    year = now.year
    month = now.month
    time.sleep(3)
    print("ğŸŒ¦ï¸ å¤©æ°£çˆ¬èŸ²å®Œæˆ\n")
    target_filename = f"{station}-{year}-{month}.csv"
    normalize_weather_filename(download_dir, target_filename)
    print("ğŸ“‚ ä¸‹è¼‰å®Œæˆï¼Œç›®å‰è³‡æ–™å¤¾å…§å®¹ï¼š", os.listdir(download_dir))



def sort(path):
    now = datetime.now()
    year = now.year
    month = now.month
    day = now.day
    weather_path = f'{path}/467410-{year}-{month}.csv'
    df_weather = pd.read_csv(weather_path)  
    df_weather.columns = df_weather.iloc[0]
    df_weather = df_weather.drop(0).reset_index(drop=True)
    invalid_rows = df_weather[df_weather["StnPres"] == "--"].index.tolist()
    print("StnPres ç‚º '--' çš„åˆ—ç´¢å¼•ï¼š", invalid_rows)
    df_weather = df_weather.drop(columns=['ObsTime', 'StnPresMaxTime', 'StnPresMinTime', 'T Max Time','T Min Time', 'RHMinTime','WGustTime', 
                                        'PrecpMax10Time', 'PrecpMax60Time','UVI Max Time'])
    df_weather.columns
    #è©²æœˆä»½å¤©æ°£ä¸è¶³7å¤©
    if invalid_rows and invalid_rows[0] < 7:
        print("âš  åµæ¸¬åˆ°å‰ 7 æ—¥å‡ºç¾ '--'ï¼Œè‡ªå‹•æ”¹è®€ä¸Šä¸€å€‹æœˆ")
        # å–ä¸Šå€‹æœˆæœˆä»½
        last_month_day = now.replace(day=1) - timedelta(days=1)
        last_month_year = last_month_day.year
        last_month = last_month_day.month
        # è‡ªå‹•ç”¢ç”Ÿä¸Šä¸€å€‹æœˆçš„è·¯å¾‘ï¼ˆè£œé›¶ï¼‰
        last_weather_path = f'crawler_DF/467410-{last_month_year}-{last_month:02d}.csv'
        df_last_weather = pd.read_csv(last_weather_path)
        df_last_weather.columns = df_weather.iloc[0]
        df_last_weather = df_weather.drop(0).reset_index(drop=True)
        df_last_weather = df_weather.drop(columns=['ObsTime', 'StnPresMaxTime', 'StnPresMinTime', 'T Max Time','T Min Time', 'RHMinTime','WGustTime', 
                                            'PrecpMax10Time', 'PrecpMax60Time','UVI Max Time'])
        
        
        df_cur_valid = df_weather[df_weather["StnPres"] != "--"]
        # å…ˆå˜—è©¦æ‹¿æœ¬æœˆæœ€å¾Œå¹¾å¤©ï¼ˆæœ€å¤š 7 ç­†ï¼‰
        cur_part = df_cur_valid.tail(7)
        n_cur = len(cur_part)
        df_last_valid = df_last_weather[df_last_weather["StnPres"] != "--"]
        need = 7 - n_cur
        last_part = df_last_valid.tail(need)

        # æ³¨æ„é †åºï¼šå…ˆèˆŠæœˆã€å†æ–°æœˆï¼Œæ™‚é–“ä¸Šæ¯”è¼ƒåˆç†
        week_block = pd.concat([last_part, cur_part], ignore_index=True)
    else: 
        print("âœ” è³‡æ–™å®Œæ•´ï¼Œä½¿ç”¨æœ¬æœˆæœ€å¾Œ 7 ç­†æœ‰æ•ˆè³‡æ–™")
        df_cur_valid = df_weather[df_weather["StnPres"] != "--"]
        week_block = df_cur_valid.tail(7)
    week_block = week_block.apply(pd.to_numeric, errors="coerce")
    df_weak_weather = week_block.mean(numeric_only=True)
    df_weak_weather
    data = {
        'è¡Œæ”¿å€': [
            'æ–°ç‡Ÿå€','é¹½æ°´å€','ç™½æ²³å€','æŸ³ç‡Ÿå€','å¾Œå£å€','æ±å±±å€','éº»è±†å€','ä¸‹ç‡Ÿå€','å…­ç”²å€','å®˜ç”°å€',
            'å¤§å…§å€','ä½³é‡Œå€','å­¸ç”²å€','è¥¿æ¸¯å€','ä¸ƒè‚¡å€','å°‡è»å€','åŒ—é–€å€','æ–°åŒ–å€','å–„åŒ–å€','æ–°å¸‚å€',
            'å®‰å®šå€','å±±ä¸Šå€','ç‰äº•å€','æ¥ è¥¿å€','å—åŒ–å€','å·¦é®å€','ä»å¾·å€','æ­¸ä»å€','é—œå»Ÿå€','é¾å´å€',
            'æ°¸åº·å€','æ±å€','å—å€','åŒ—å€','ä¸­è¥¿å€','å®‰å—å€','å®‰å¹³å€'
        ],
        'å€åˆ¥': [
            '67000010','67000020','67000030','67000040','67000050','67000060','67000070','67000080','67000090','67000100',
            '67000110','67000120','67000130','67000140','67000150','67000160','67000170','67000180','67000190','67000200',
            '67000210','67000220','67000230','67000240','67000250','67000260','67000270','67000280','67000290','67000300',
            '67000310','67000320','67000330','67000340','67000350','67000360','67000370'
        ]
    }
    TW_year = year - 1911
    bucket_path = f'{path}/bucket_{TW_year}.csv'
    df_bucket = pd.read_csv(bucket_path)
    df_map = pd.DataFrame(data)
    # ç”¨ merge ä¾ç…§ã€Œå€åˆ¥ã€ä¾†å°æ‡‰è¡Œæ”¿å€
    df_bucket["å€åˆ¥"] = df_bucket["å€åˆ¥"].astype(str).str.strip()
    df_bucket = df_bucket.merge(df_map, on="å€åˆ¥", how="left")

    # è‹¥ä½ æƒ³æŠŠè¡Œæ”¿å€æ”¾å‰é¢ã€æˆ–æŠŠå€åˆ¥ä¸Ÿæ‰ï¼š
    df_bucket = df_bucket.drop(columns=["Seq","ç¸£å¸‚","å€åˆ¥","ç›£æ¸¬é€±æœŸ"])
    df_bucket = df_bucket.tail(10).reset_index(drop=True)
    df_bucket = df_bucket[["è¡Œæ”¿å€","é™½æ€§ç‡","ç¸½åµç²’æ•¸"]]
    df_bucket 
    if isinstance(df_weak_weather, pd.Series):
        df_weak_weather_df = df_weak_weather.to_frame().T
    else:
        df_weak_weather_df = df_weak_weather
    # åŠ ä¸€å€‹å‡éµ key=1ï¼Œåš cross join
    df_bucket["key"] = 1
    df_weak_weather_df["key"] = 1

    df_merged = df_bucket.merge(df_weak_weather_df, on="key").drop(columns=["key"])

    # è¼¸å‡º CSV
    output_path = f"{path}/week_data.csv"
    df_merged.to_csv(output_path, index=False, encoding="utf-8-sig")
###########################################
# Main: é¸æ“‡è¦è·‘å“ªä¸€æ®µ
###########################################
if __name__ == "__main__":

    # ä¸€æ¬¡å»ºç«‹ driver
    driver, wait, download_dir = init_driver("crawler_DF")
    # ä½ å¯ä»¥é¸æ“‡è¦è·‘å“ªäº›
    crawl_bucket(driver, wait, download_dir)                     # ğŸª£ èª˜åµæ¡¶
    crawl_weather(driver, wait,download_dir, "467410", "2024-10")  # ğŸŒ¦ï¸ å¤©æ°£è³‡æ–™
    print(download_dir)
    sort(download_dir)
    # driver.quit()
